% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/sample_mcmc.R
\name{sampler}
\alias{sampler}
\title{MCMC sampling for pre-specified latent state dimension M}
\usage{
sampler(dat, model_options, mcmc_options)
}
\arguments{
\item{dat}{binary data matrix (row for observations and column for dimensions)}

\item{model_options}{Specifying assumed model options:
\itemize{
\item \code{n} The number of subjects.
\item \code{t_max} The maximum (guessed) number of clusters in the data during
the posterior inference
\item \code{m_max} For a model with pre-specified number of factors, \code{m_max};
In an infinite dimension model, \code{m_max} is
the maximum (guessed) latent state dimension during the posterior inference
(see \code{\link{slice_sampler}}); one can increase this number if
this pacakge recommends so in the printed message;
\item \code{a_theta, a_psi} hyperparameters for true and false positive rates;
a_theta and a_psi are both a vector of length two.
\item \code{a_alpha, b_alpha} Just for infinite latent state dimension model  -
Gamma hyperparameter for the hyperprior on \code{alpha}.
(See \code{\link{slice_sampler}})
\item \code{log_v} The charaster string representing the prior
distribution for the number of true clusters, e.g.,
\code{"function(k) {log(0.1) + (k-1)*log(0.9)}"}. We pre-computed
log of the coefficients in Mixture of Finite Mixtures
(Miller and Harrison, 2017, JASA). Use this code:
\code{coefficients(eval(parse(text=model_options0$log_pk)),
model_options0$gamma,
model_options0$n,
model_options0$t_max+1)}
}
The following are used if one needs to pre-specify a few unknown parameters to
their respective true or other values
\itemize{
\item \code{Q} Q matrix
\item \code{theta} a vector of true positive rates
\item \code{psi} a vector of false positive rates
\item \code{p} a vector of latent state prevalences
\item \code{alpha} For pre-specified latent state dimension, the hyperparameter
for \code{Beta(alpha/m_max,1)} (can set to \code{m_max});
For infinite dimension model, the hyperparameter for IBP (can set to 1).
}
Options for specifying data, sample size, max cluster number,
coefficients in MFM (Miller and Harrison 2017 JASA), Gamma parameter in the MFM
Dirchlet prior, number of intermediate Gibbs scan to arrive at the launch state,
and other hyperparamter specification if needed, \code{n_total} for total number of
MCMC iterations and \code{n_keep} for the number of samples kept for posterior inference.
Note that the options involve other parameters for sampling hyperparameters such as
alpha in the Indian Buffet Process.}

\item{mcmc_options}{Options for MCMC sampling:
\itemize{
\item \code{n_total} total number of MCMC iterations
\item \code{n_keep} number of iterations kept
\item \code{n_split} the number of restricted Gibbs scan to arrive at a launch state;
see \link{restricted_gibbs}
\item \code{print_mod} print intermediate model fitting information
\item \code{constrained} update the Q matrix with identifiability constraints (if \code{TRUE});
otherwise, set to \code{FALSE}.
\item \code{block_update_H} update rows of H (if \code{TRUE}) or not
(if \code{NULL} or \code{FALSE} - must be so for \code{\link{slice_sampler}}).
\item \code{block_update_Q} update columns of Q (if \code{TRUE}) or not
(if \code{NULL} or \code{FALSE} - must be so for \code{\link{slice_sampler}}).
Then no identifiability constraint is imposed upon Q at any iterations.
}}
}
\value{
posterior samples for quantities of interest.
It is a list comprised of the following elements:
\itemize{
\item \code{t_samp}
\item \code{z_samp}
\item \code{N_samp}
\item \code{keepers} indices of MCMC samples kept for inference;
\item \code{H_star_samp}
\item \code{H_star_merge_samp}
\item \code{alpha_samp}
} The following are recorded if they are not fixed in a priori:
\itemize{
\item \code{Q_samp}
\item \code{Q_merge_samp}
\item \code{theta_samp}
\item \code{psi_samp}
\item \code{p_samp}
}
}
\description{
This function performs MCMC sampling with user-specified options.
NB: 1) add flexibility to specify other parameters as fixed. 2) sample component-specific
parameters. 3) sample other model parameters. 4) add timing and printing functionality.
5) add posterior summary functions.
6) edit verbose contents.
}
\examples{
#\dontrun{
rm(list=ls())
library(rewind)
library(matrixStats)
library(ars)

# color palette for heatmaps:
YlGnBu5   <- c('#ffffd9','#c7e9b4','#41b6c4','#225ea8','#081d58',"#092d94")
hmcols    <- colorRampPalette(YlGnBu5)(256)

# simulate data:
L0 <- 100
options_sim0  <- list(N = 100,  # sample size.
                      M = 3,   # true number of machines.
                      L = L0,   # number of antibody landmarks.
                      K = 2^3,    # number of true components.,
                      theta = rep(0.8,L0), # true positive rates
                      psi   = rep(0.1,L0), # false positive rates
                      alpha1 = 1 # half of the people have the first machine.
)

image(simulate_data(options_sim0,SETSEED = TRUE)$datmat,col=hmcols)
simu     <- simulate_data(options_sim0, SETSEED=TRUE)
simu_dat <- simu$datmat

#
# specifying options
#

# check BayesianMixtures.jl for how options were set.

# model options:
model_options0 <- list(
  n   = nrow(simu_dat),
  t_max  = 40,
  m_max  = 5,
  b  = 1, # Dirichlet hyperparameter; in the functions above,
  # we used "b" - also can be called "gamma"!.
  #Q  = simu$Q,
  a_theta = c(9,1),
  a_psi   = c(1,9),
  #theta = options_sim0$theta,
  #psi   = options_sim0$psi,
  #alpha   = options_sim0$M,
  #p_both      = rep(0.5,3),#,c(0.5,0.5^2,0.5^3,0.5^4,0.5^5)
  log_pk = "function(k) {log(0.1) + (k-1)*log(0.9)}"# Geometric(0.1).
  #Prior for the number of components.
)



# pre-compute the log of coefficients in MFM:
model_options0$log_v<-mfm_coefficients(eval(parse(text=model_options0$log_pk)),
                                       model_options0$b,
                                       model_options0$n,
                                       model_options0$t_max+1)

# mcmc options:
mcmc_options0 <- list(
  n_total = 200,
  n_keep  = 200,
  n_split = 5,
  print_mod = 10,
  constrained = TRUE, # <-- need to write a manual about when these options are okay.
  block_update_H = TRUE,
  block_update_Q = !TRUE,
  hmcols = hmcols
)


#
# run posterior algorithm for simulated data:
#
out <- sampler(simu_dat,model_options0,mcmc_options0)

#
# Posterior summaries: (Question, how to reconcile differences of the minimizing
#    elements for, H, Q, Z?)
#

# co-clustering:
comat <- coclust_mat(nrow(simu_dat),out$z_samp,mcmc_options0$n_keep)
image(1:options_sim0$N,1:options_sim0$N, comat,
      xlab="Subjects",ylab="Subjects",col=hmcols,main="co-clustering")
for (k in 1:options_sim0$K){
  abline(h=cumsum(rle(simu$Z)$lengths)[k]+0.5,lty=2)
  abline(v=cumsum(rle(simu$Z)$lengths)[k]+0.5,lty=2)
}

trans_squared_error <- function(A,B){
  norm(A\%*\%t(A) - B\%*\%t(B),"F")
}

myentropy <- function(z){
  n <- length(z)
  tb <- table(z)
  log(n) - sum(tb*log(tb))/n
}

z2comat <- function(z){
  n <- length(z)
  res <- matrix(NA,nrow=n,ncol=n)
  for (i in 1:(n-1)){
    for (j in (i+1):n){
      res[i,j] <- res[j,i] <- (z[i]==z[j])+0
    }
  }
  diag(res) <- 1
  res
}

## summarize partition C:
## Approach 0: the maximum a posterior: issue - the space is huge.
## Approach 1: using the entropy to get posterior median and the credible interval;
## Approach 2: minimizing the least squared error between the co-clustering indicators
##            and the posterior co-clustering probabilities;
## Approach 3: Wade - estimate (greedy) the best partition using posterior expected loss (VI).

nsamp_C <- dim(out$z_samp)[2]
z_Em <- rep(NA,nsamp_C)
## approach 2:
for (iter in 1:nsamp_C){
  z_Em[iter] <- norm(z2comat(out$z_samp[,iter])-comat,"F")
}
ind_dahl <- which.min(z_Em)
plot(z_Em,type="l",main="|S-E[S|Y]|")
image(1:options_sim0$N,1:options_sim0$N,
      z2comat(out$z_samp[,ind_dahl]),col=hmcols,
      main="clustering at the minimized Z")
for (k in 1:options_sim0$K){
  abline(h=cumsum(rle(simu$Z)$lengths)[k]+0.5,lty=2)
  abline(v=cumsum(rle(simu$Z)$lengths)[k]+0.5,lty=2)
}

#3 approach 1:
for (iter in 1:nsamp_C){
  z_Em[iter] <- myentropy(out$z_samp[,iter])
}

bd <- HDInterval::hdi(z_Em)
z_Em[which(z_Em==bd[1])]
z_Em[which(z_Em==bd[2])]
which(z_Em == median(z_Em))

# coarser:
ind_coarser <- which(z_Em==bd[1])
image(1:options_sim0$N,1:options_sim0$N,
      z2comat(out$z_samp[,ind_coarser[1]]),col=hmcols,
      main="coarser clustering at the lower entropy quantile")
for (k in 1:options_sim0$K){
  abline(h=cumsum(rle(simu$Z)$lengths)[k]+0.5,lty=2)
  abline(v=cumsum(rle(simu$Z)$lengths)[k]+0.5,lty=2)
}

# refined:
ind_refined <- which(z_Em==bd[2])
image(1:options_sim0$N,1:options_sim0$N,
      z2comat(out$z_samp[,ind_refined]),col=hmcols,
      main="granular clustering at the upper entropy quantile")
for (k in 1:options_sim0$K){
  abline(h=cumsum(rle(simu$Z)$lengths)[k]+0.5,lty=2)
  abline(v=cumsum(rle(simu$Z)$lengths)[k]+0.5,lty=2)
}

# median:
ind_median <- which(abs(z_Em-median(z_Em))<1e-3)
image(1:options_sim0$N,1:options_sim0$N,
      z2comat(out$z_samp[,ind_median]),col=hmcols,
      main="median entropy clustering")

for (k in 1:options_sim0$K){
  abline(h=cumsum(rle(simu$Z)$lengths)[k]+0.5,lty=2)
  abline(v=cumsum(rle(simu$Z)$lengths)[k]+0.5,lty=2)
}


## summarize Q (first need to transpose it)
##
##
## Approach 1:  compute the error |QQ' - E[QQ'|Y]|_Frobneious
## Approach 2: compute the marginal co-positive probability
##            P(\\sum_m Q_ml >= 1, \\sum_m Q_ml' >= 1) -
##             This is invariant to the relabeling of latent states, or cluster labels.
nsamp_Q <- dim(out$Q_merge_samp)[3]
EQQ <- matrix(0,nrow=dim(out$Q_merge_samp)[2], ncol=dim(out$Q_merge_samp)[2])
for (iter in 1:nsamp_Q){
  A <- t(out$Q_merge_samp[,,iter])
  EQQ <- (A\%*\%t(A)+EQQ*(iter-1))/iter
}
Q_Em <- rep(NA,nsamp_Q)
for (iter in 1:nsamp_Q){
  A <- t(out$Q_merge_samp[,,iter])
  Q_Em[iter] <- norm(A\%*\%t(A) - EQQ,"F")
}
plot(Q_Em,type="o",main="||QQ'-E[QQ'|Y]||")

# comparing true Q to sampled Q:
#ind_of_Q <- ind_median[which.min(Q_Em[ind_median])]#which.max(table(pats))
ind_of_Q <- which(Q_Em==min(Q_Em))#which.max(table(pats))
#ind_post_mode <- mcmc_options0$n_total#which.max(table(pats))
#pdf("diagnosticsQ.pdf",width=12,height=6)
par(mfrow=c(1,4))
image(simu$datmat,main="Data",col=hmcols)
image(simu$xi,main="True presence/absence of proteins)",col=hmcols)
image(order_mat_byrow(simu$Q)$res,main="True Q (ordered)",col=hmcols)
Q_merged <- out$Q_merge_samp[,,ind_of_Q]
image(order_mat_byrow(Q_merged[rowSums(Q_merged)!=0,,drop=FALSE])$res,
      main="Sampled Q (merged & ordered)",col=hmcols)
#dev.off()

## summarize H*:
##
## If we compare it to H, then the clustering matters too, not
## just H^*.
##
##  compute the error |HH' - E[HH'|Y]|_Frobneious
##
nsamp_H <- dim(out$z_samp)[2]
# H_Em <- rep(NA,nsamp_H)
# for (iter in 1:nsamp_H){
#   H_Em[iter] <- trans_squared_error(out$H_star_merge_samp[out$z_samp[,iter],,iter],
#                                     simu$H_star[simu$Z,])
# }
# plot(H_Em,type="o",main="|HH'-E[H0H0'|Y]|")

H_res <- matrix(0,nrow=nrow(simu$datmat),ncol=model_options0$m_max)
H_pat_res <- matrix(0,nrow=nrow(simu$datmat),ncol=length(ind_of_Q))
for (l in seq_along(ind_of_Q)){
  tmp_mylist <- out$mylist_samp[,ind_of_Q[l]]
  tmp <- out$H_star_merge_samp[tmp_mylist[out$z_samp[,ind_of_Q[l]]],,ind_of_Q[l]]
  H_pat_res[,l] <- bin2dec_vec(tmp,LOG=FALSE)
  #H_res <- (out$H_star_merge_samp[out$z_samp[,ind_of_Q[l]],,ind_of_Q[l]] + H_res*(l-1))/l
}
apply(H_pat_res,1,table)
# ind_of_Q[which(H_pat_res[1,]==0)]
#
# image(simu$Q)
# image(out$Q_merge_samp[,,24])
# image(out$H_star_merge_samp[,,24])
# image(out$z_samp[1,24])
# image(out$H_star_samp[,,24])
# out$H_star_samp[1,,24]

Q_res <- matrix(0,nrow=model_options0$m_max,ncol=ncol(simu$Q))
for (l in seq_along(ind_of_Q)){
  Q_res <- (out$Q_merge_samp[,,ind_of_Q[l]] + Q_res*(l-1))/l
}

#min_H_Em <- which.min(Q_Em) # select the one with minimal squared error.
image(1:options_sim0$N,1:model_options0$m_max,
      H_res,
      col=hmcols,xlab="subjects",ylab="latent states")
for (k in 1:options_sim0$K){
  abline(v=cumsum(rle(simu$Z)$lengths)[k]+0.5,lty=2)
}
#}
}
